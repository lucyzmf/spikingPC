{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXvTEGS2ixCf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the spiking neural network on the MNIST dataset with convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o471k68WkYQg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ep5YKEvCiqF3",
    "outputId": "74c82417-2d42-48a1-a82f-fadbf862ebd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p00ywlX3a-QF",
    "outputId": "008d0f0c-bb50-4a38-d02a-4309768e57c2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "epoch:  0 . Loss:  800.4718017578125 . tr Acc:  0.41514  ,ts Acc: 57.479\n",
      "epoch:  1 . Loss:  560.6456909179688 . tr Acc:  0.6280125  ,ts Acc: 67.029\n",
      "epoch:  2 . Loss:  478.5580749511719 . tr Acc:  0.6910241666666667  ,ts Acc: 71.35900000000001\n",
      "epoch:  3 . Loss:  427.5786437988281 . tr Acc:  0.7268116666666666  ,ts Acc: 74.1255\n",
      "epoch:  4 . Loss:  390.7110900878906 . tr Acc:  0.7520683333333333  ,ts Acc: 76.46849999999999\n",
      "epoch:  5 . Loss:  360.2369689941406 . tr Acc:  0.7712825  ,ts Acc: 77.4235\n",
      "epoch:  6 . Loss:  337.2833557128906 . tr Acc:  0.7847708333333333  ,ts Acc: 78.8025\n",
      "epoch:  7 . Loss:  317.8880920410156 . tr Acc:  0.7965166666666667  ,ts Acc: 80.067\n",
      "epoch:  8 . Loss:  299.4064025878906 . tr Acc:  0.8084441666666667  ,ts Acc: 81.074\n",
      "epoch:  9 . Loss:  282.916015625 . tr Acc:  0.81756  ,ts Acc: 81.9835\n",
      "epoch:  10 . Loss:  268.64422607421875 . tr Acc:  0.8260175000000001  ,ts Acc: 82.5635\n",
      "epoch:  11 . Loss:  255.95419311523438 . tr Acc:  0.8335425000000001  ,ts Acc: 83.353\n",
      "epoch:  12 . Loss:  242.9864501953125 . tr Acc:  0.8415033333333333  ,ts Acc: 84.0085\n",
      "epoch:  13 . Loss:  233.2805633544922 . tr Acc:  0.8465866666666666  ,ts Acc: 84.3735\n",
      "epoch:  14 . Loss:  223.1736297607422 . tr Acc:  0.8540275  ,ts Acc: 85.0755\n",
      "epoch:  15 . Loss:  214.0602264404297 . tr Acc:  0.859025  ,ts Acc: 85.447\n",
      "epoch:  16 . Loss:  206.62960815429688 . tr Acc:  0.8641583333333334  ,ts Acc: 85.8175\n",
      "epoch:  17 . Loss:  199.39743041992188 . tr Acc:  0.8685366666666666  ,ts Acc: 86.3365\n",
      "epoch:  18 . Loss:  192.0386505126953 . tr Acc:  0.8724633333333334  ,ts Acc: 86.74249999999999\n",
      "epoch:  19 . Loss:  186.71902465820312 . tr Acc:  0.8752541666666667  ,ts Acc: 86.9565\n",
      "epoch:  20 . Loss:  181.1862335205078 . tr Acc:  0.8790716666666667  ,ts Acc: 87.2085\n",
      "epoch:  21 . Loss:  176.4148406982422 . tr Acc:  0.8820716666666667  ,ts Acc: 87.486\n",
      "epoch:  22 . Loss:  172.06593322753906 . tr Acc:  0.8846925000000001  ,ts Acc: 87.7705\n",
      "epoch:  23 . Loss:  167.0049591064453 . tr Acc:  0.8877041666666666  ,ts Acc: 87.9485\n",
      "epoch:  24 . Loss:  163.0979461669922 . tr Acc:  0.8899225  ,ts Acc: 88.17999999999999\n",
      "epoch:  25 . Loss:  158.74612426757812 . tr Acc:  0.8925416666666667  ,ts Acc: 88.425\n",
      "epoch:  26 . Loss:  155.773681640625 . tr Acc:  0.8938183333333333  ,ts Acc: 88.40549999999999\n",
      "epoch:  27 . Loss:  153.1468505859375 . tr Acc:  0.8957075  ,ts Acc: 88.73400000000001\n",
      "epoch:  28 . Loss:  150.13388061523438 . tr Acc:  0.8972033333333332  ,ts Acc: 88.738\n",
      "epoch:  29 . Loss:  147.49844360351562 . tr Acc:  0.8982041666666667  ,ts Acc: 88.886\n",
      "epoch:  30 . Loss:  144.80555725097656 . tr Acc:  0.9003291666666666  ,ts Acc: 88.951\n",
      "epoch:  31 . Loss:  141.9029541015625 . tr Acc:  0.9018741666666666  ,ts Acc: 89.2735\n",
      "epoch:  32 . Loss:  139.6398162841797 . tr Acc:  0.9031533333333333  ,ts Acc: 89.0845\n",
      "epoch:  33 . Loss:  137.77316284179688 . tr Acc:  0.9045475  ,ts Acc: 89.3155\n",
      "epoch:  34 . Loss:  135.91221618652344 . tr Acc:  0.9060066666666667  ,ts Acc: 89.3805\n",
      "epoch:  35 . Loss:  134.09994506835938 . tr Acc:  0.9068266666666667  ,ts Acc: 89.253\n",
      "epoch:  36 . Loss:  133.01376342773438 . tr Acc:  0.90757  ,ts Acc: 89.4435\n",
      "epoch:  37 . Loss:  130.76661682128906 . tr Acc:  0.9087541666666666  ,ts Acc: 89.627\n",
      "epoch:  38 . Loss:  128.6359405517578 . tr Acc:  0.9099833333333334  ,ts Acc: 89.7855\n",
      "epoch:  39 . Loss:  127.4106216430664 . tr Acc:  0.9113883333333334  ,ts Acc: 89.773\n",
      "epoch:  40 . Loss:  126.67913055419922 . tr Acc:  0.9114683333333333  ,ts Acc: 89.916\n",
      "epoch:  41 . Loss:  125.02005767822266 . tr Acc:  0.912335  ,ts Acc: 89.916\n",
      "epoch:  42 . Loss:  122.76753997802734 . tr Acc:  0.9136958333333334  ,ts Acc: 90.027\n",
      "epoch:  43 . Loss:  121.78071594238281 . tr Acc:  0.9145791666666667  ,ts Acc: 90.147\n",
      "epoch:  44 . Loss:  121.44448852539062 . tr Acc:  0.9151166666666667  ,ts Acc: 90.0615\n",
      "epoch:  45 . Loss:  120.08108520507812 . tr Acc:  0.91572  ,ts Acc: 89.963\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# import matplotlib\n",
    "# TODO: code cleaning\n",
    "T = 20\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "batch_size = 128\n",
    "train_dataset = dsets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',train=False,transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "n_iters = 300000\n",
    "decay = 0.1  # neuron decay rate\n",
    "thresh = 0.5  # neuronal threshold\n",
    "lens = 0.3  # hyper-parameters of approximate function\n",
    "num_epochs = 150  # n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "'''\n",
    "STEP 3a: CREATE spike MODEL CLASS\n",
    "'''\n",
    "\n",
    "b_j0 = 0.01  # neural threshold baseline\n",
    "tau_m = 20  # ms membrane potential constant\n",
    "R_m = 1  # membrane resistance\n",
    "dt = 1  #\n",
    "gamma = .5  # gradient scale\n",
    "\n",
    "def gaussian(x,mu=0.,sigma=.5):\n",
    "    return torch.exp(-((x-mu)**2)/(2*sigma**2))/torch.sqrt(2*torch.tensor(math.pi))/sigma\n",
    "# define approximate firing function\n",
    "class ActFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = membrane potential- threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()  # is firing ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        temp = gaussian(input,mu=0.,sigma=lens)*1.15 \\\n",
    "            - gaussian(input,mu=lens,sigma=1.75*lens)*.15 \\\n",
    "                - gaussian(input,mu=-lens,sigma=1.75*lens)*.15\n",
    "        return gamma * grad_input * temp.float()\n",
    "\n",
    "\n",
    "act_fun_adp = ActFun_adp.apply\n",
    "# membrane potential update\n",
    "\n",
    "\n",
    "def mem_update_NU_adp(inputs, mem, spike, tau_adp, tau_m,b, dt=1, isAdapt=1):\n",
    "    alpha = torch.exp(-1. * dt / tau_m).cuda()\n",
    "    ro = torch.exp(-1. * dt / tau_adp).cuda()\n",
    "    if isAdapt:\n",
    "        beta = 1.8\n",
    "    else:\n",
    "        beta = 0.\n",
    "    b = ro * b + (1. - ro) * spike\n",
    "    B = b_j0 + beta * b\n",
    "\n",
    "    mem = mem * alpha + (1 - alpha) * R_m * inputs - B * spike * dt\n",
    "    inputs_ = mem - B\n",
    "    spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "    # spike = F.relu(inputs)\n",
    "    return mem, spike, B, b\n",
    "\n",
    "def integrator(inputs,mem,tau_m):\n",
    "    # alpha = torch.exp(-1. * dt / tau_m).cuda()\n",
    "    alpha =tau_m\n",
    "    # mem = mem * alpha + (1 - alpha) * R_m * inputs\n",
    "    mem = mem  + alpha* inputs\n",
    "    return mem\n",
    "\n",
    "def std_cross_entropy(x, y):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    loss = log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "class RNN_custom(nn.Module):\n",
    "    def __init__(self, input_size, output_size,criterion=nn.NLLLoss(),hidden_size = [128,10],filters=[15,40],kernels = [5,5],is_rec = 0):\n",
    "        super(RNN_custom, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(784, hidden_size[0])\n",
    "        \n",
    "        self.h2o = nn.Linear(hidden_size[1], output_size)\n",
    "\n",
    "        self.h2r = nn.Linear(hidden_size[0], hidden_size[1]-10)\n",
    "\n",
    "        self.r2r = nn.Linear(hidden_size[1]-10, hidden_size[1]-10)\n",
    "        self.r2p = nn.Linear(hidden_size[1]-10, 10)\n",
    "        self.p2r = nn.Linear(10, hidden_size[1]-10)\n",
    "        self.p2p = nn.Linear(10, 10)\n",
    "\n",
    "\n",
    "\n",
    "        self.tau_adp_h = nn.Parameter(torch.Tensor(hidden_size[0]))\n",
    "        self.tau_adp_h2 = nn.Parameter(torch.Tensor(hidden_size[1]))\n",
    "        self.tau_adp_o = nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        self.tau_m_h = nn.Parameter(torch.Tensor(hidden_size[0]))\n",
    "        self.tau_m_h2 = nn.Parameter(torch.Tensor(hidden_size[1]))\n",
    "        self.tau_m_o = nn.Parameter(torch.Tensor(output_size))\n",
    "\n",
    "        # nn.init.orthogonal_(self.h2h.weight)\n",
    "        nn.init.xavier_uniform_(self.i2h.weight)\n",
    "        nn.init.xavier_uniform_(self.h2o.weight)\n",
    "        nn.init.xavier_uniform_(self.h2r.weight)\n",
    "        nn.init.xavier_uniform_(self.r2r.weight)\n",
    "        nn.init.xavier_uniform_(self.r2p.weight)\n",
    "        nn.init.xavier_uniform_(self.p2p.weight)\n",
    "        nn.init.xavier_uniform_(self.p2r.weight)\n",
    "        # nn.init.xavier_uniform_(self.d1.weight)\n",
    "\n",
    "        nn.init.constant_(self.i2h.bias, 0)\n",
    "        # nn.init.constant_(self.h2h.bias, 0)\n",
    "        # nn.init.constant_(self.d1.bias, 0)\n",
    "\n",
    "        nn.init.constant_(self.tau_adp_h, 200)\n",
    "        nn.init.constant_(self.tau_adp_h2, 200)\n",
    "        nn.init.constant_(self.tau_adp_o, 200)\n",
    "\n",
    "        nn.init.constant_(self.tau_m_h, 20)\n",
    "        nn.init.constant_(self.tau_m_h2, 20)\n",
    "        nn.init.constant_(self.tau_m_o, 0.5)\n",
    "\n",
    "        self.b_c = self.b_h = self.b_o = 0\n",
    "\n",
    "        self.is_rec = is_rec\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        batch_size, seq_num, input_dim = input.shape\n",
    "        self.b_c1=self.b_c2 = self.b_h= self.b_h2 = self.b_o = b_j0\n",
    "\n",
    "        h1_mem =torch.rand(batch_size, self.hidden_size[0]).cuda()\n",
    "        h1_spike =torch.zeros(batch_size, self.hidden_size[0]).cuda()\n",
    "\n",
    "        h2_mem =torch.rand(batch_size, self.hidden_size[1]).cuda()\n",
    "        h2_spike =torch.zeros(batch_size, self.hidden_size[1]).cuda()\n",
    "\n",
    "        h2o_mem = torch.rand(batch_size, self.output_size).cuda()\n",
    "        sum_spike_out = h2o_spike = torch.zeros(batch_size, self.output_size).cuda()\n",
    "            \n",
    "        hidden_spike_ = []\n",
    "        h2o_spike_ = []\n",
    "        theta_h_ = []\n",
    "        theta_o_ = []\n",
    "        hidden_mem_ = []\n",
    "        h2o_mem_ = []\n",
    "        output_ = []\n",
    "        loss = 0\n",
    "        predictions = []\n",
    "        std_ = []\n",
    "\n",
    "        I_h = []\n",
    "        for i in range(T):\n",
    "            x = input.view(-1,28*input_dim)\n",
    "            x =  F.dropout(x,0.2,training=True)\n",
    "            ####################################################################\n",
    "            h_input = self.i2h(x)\n",
    "            \n",
    "            h1_mem, h1_spike, theta_h, self.b_h = mem_update_NU_adp(h_input,h1_mem, h1_spike, self.tau_adp_h,self.tau_m_h, self.b_h)\n",
    "            r_input = self.h2r(h1_spike)+self.r2r(h2_spike[:,10:])+self.p2r(h2_spike[:,:10])\n",
    "            # print(self.h2r(h1_spike).shape, self.r2r(h2_spike[:,10:]).shape,self.p2r(h2_spike[:,:10]).shape)\n",
    "            # r_input = F.avg_pool1d(x,10,8)[:,0,:]+self.r2r(h2_spike[:,10:])+self.p2r(h2_spike[:,:10])\n",
    "            p_input = self.p2p(h2_spike[:,:10])+self.r2p(h2_spike[:,10:])\n",
    "\n",
    "            # \n",
    "\n",
    "            # h2_input_pad = F.pad(source, pad=(10, 0, 0,0))\n",
    "            # print(p_input.shape,r_input.shape)\n",
    "            h2_input = torch.cat((p_input,r_input),dim=1)#h2_input_pad + self.h2h(h2_spike)\n",
    "\n",
    "            h2_mem, h2_spike, theta_h2, self.b_h2 = mem_update_NU_adp(h2_input,h2_mem, h2_spike, self.tau_adp_h2,self.tau_m_h2, self.b_h2)\n",
    "            I_h.append(p_input.data.cpu().numpy())\n",
    "\n",
    "            # h2o_mem, h2o_spike, theta_o, self.b_o = mem_update_NU_adp(self.h2o(h2_spike), h2o_mem, h2o_spike, self.tau_adp_o,self.tau_m_o, self.b_o)\n",
    "            # h2o_input = self.h2o(h2_spike)\n",
    "            # print(h2_spike.shape)\n",
    "            h2o_input = h2_spike[:,:10]\n",
    "            h2o_mem = integrator(h2o_input,h2o_mem,self.tau_m_o)\n",
    "            # h2o_spike = h2_spike[:,-10:]\n",
    "\n",
    "            # sum_spike_out =sum_spike_out + h2o_spike\n",
    "            output_sumspike = F.log_softmax(h2o_mem,dim=1)\n",
    "            output_.append(output_sumspike.data.cpu().numpy())\n",
    "            if i >=0:\n",
    "                std_mat = torch.tensor(np.array(output_)[-3:,:,:])\n",
    "                output_std = torch.std(std_mat,dim=0)\n",
    "                std_.append(output_std.data.cpu().numpy())\n",
    "                # print(output_std.shape,self.criterion(output_sumspike, labels))\n",
    "\n",
    "                loss += self.criterion(output_sumspike, labels)/T\n",
    "                predictions.append(output_sumspike.data.cpu().numpy())\n",
    "            # loss = self.criterion(output_sumspike, labels)\n",
    "\n",
    "            hidden_spike_.append(h1_spike.data.cpu().numpy())\n",
    "            hidden_mem_.append(h1_mem.data.cpu().numpy())\n",
    "\n",
    "            h2o_spike_.append(h2o_spike.data.cpu().numpy())\n",
    "            h2o_mem_.append(h2o_mem.data.cpu().numpy())\n",
    "\n",
    "            theta_h_.append(theta_h2.data.cpu().numpy())\n",
    "            # theta_o_.append(theta_o.data.cpu().numpy())\n",
    "            \n",
    "            \n",
    "                \n",
    "        predictions = torch.tensor(np.exp(predictions)) \n",
    "        return [predictions,h2o_mem_,h2o_spike_,std_],loss# output_sumspike, h1_spike, hidden_spike_, h2o_spike_, output_, hidden_mem_, h2o_mem_, theta_h_, theta_o_, I_h\n",
    "    \n",
    "    def predict(self, input, lablel):\n",
    "        prediction,_ = self.forward(input, lablel)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = [155,155+10]\n",
    "layer_dim = 1  # layer number\n",
    "output_dim = 10\n",
    "seq_dim = int(784 / input_dim)  # Number of steps to unroll\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = RNN_custom(input_dim,  output_dim,hidden_size=hidden_dim)\n",
    "#model = torch.load('./model/model_97.98-v7 (copy).pth')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 3e-3\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "base_params = [\n",
    "               model.i2h.weight, model.i2h.bias, \n",
    "               model.r2r.weight, model.r2r.bias,\n",
    "               model.r2p.weight, model.r2p.bias,\n",
    "               model.h2r.weight, model.h2r.bias,\n",
    "               model.p2r.weight, model.p2r.bias,\n",
    "               model.p2p.weight, model.p2p.bias,\n",
    "            #    model.d1.weight, model.d1.bias,\n",
    "                model.h2o.weight, model.h2o.bias]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adamax([\n",
    "    {'params': base_params},\n",
    "\n",
    "    {'params': model.tau_adp_h, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_adp_h2, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_adp_o, 'lr': learning_rate * 3},\n",
    "\n",
    "    {'params': model.tau_m_h, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_m_h2, 'lr': learning_rate * 2},\n",
    "    {'params': model.tau_m_o, 'lr': learning_rate },],\n",
    "    lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=300*10, gamma=.5)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "def train(model,num_epochs=150):\n",
    "    acc = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # if i <2:\n",
    "            images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass to get output/logits\n",
    "            predictions, train_loss = model(images, labels)\n",
    "            predictions = predictions[0]\n",
    "            _, predicted = torch.max(predictions.data, 2)\n",
    "            train_loss.backward()\n",
    "            train_loss_sum += train_loss\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            # print(predicted.shape, labels.shape)\n",
    "            predicted = predicted.t()\n",
    "            total += labels.size(0)\n",
    "            train_acc += (predicted.cpu() == labels.cpu().view(-1,1).repeat(1,int(T))).sum()\n",
    "            # train_acc += (predicted.cpu()[:,-1] == labels.cpu()).sum()\n",
    "        train_acc_np = train_acc.data.cpu().numpy()/T/total\n",
    "        \n",
    "        accuracy = test(model)\n",
    "        if accuracy > best_accuracy and accuracy > 95.:\n",
    "            # torch.save(model,'./model/cnn_srnn_'+str(accuracy)+'-v0.pth')\n",
    "            best_accuracy = accuracy\n",
    "        acc.append(accuracy)\n",
    "        # Print Loss\n",
    "        #print('epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))\n",
    "        print('epoch: ', epoch, '. Loss: ', train_loss_sum.item(), '. tr Acc: ',train_acc_np,' ,ts Acc:',accuracy)\n",
    "    return acc\n",
    "\n",
    "def test(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Iterate through test dataset\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model.predict(images, labels)\n",
    "        outputs = outputs[0]\n",
    "        _, predicted = torch.max(outputs.data, 2)\n",
    "        predicted = predicted.t()\n",
    "        # print(predicted.shape,labels.shape)\n",
    "        total += labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            correct +=  (predicted.cpu() == labels.cpu().view(-1,1).repeat(1,int(T))).sum()\n",
    "        else:\n",
    "            correct += (predicted == labels).sum()\n",
    "   \n",
    "    accuracy = 100. * correct.numpy() / total/T\n",
    "    return accuracy\n",
    "    \n",
    "def test_real(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Iterate through test dataset\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model.predict(images, labels)\n",
    "        outputs = outputs[0]\n",
    "        pred_sum = outputs.sum(axis=1)# sum along time\n",
    "        _, predicted = torch.max(pred_sum.data, 1)\n",
    "        # print(predicted.shape,labels.shape)\n",
    "        total += labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            correct += (predicted == labels).sum()\n",
    "   \n",
    "    accuracy = 100. * correct.numpy() / total\n",
    "    return accuracy\n",
    "###############################\n",
    "acc = train(model,num_epochs)\n",
    "accuracy = test(model)\n",
    "print('. Accuracy: ',accuracy)\n",
    "\n",
    "\n",
    "###################\n",
    "##  Accuracy  curve\n",
    "###################\n",
    "plt.plot(acc)\n",
    "plt.title('Learning Curve -- Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy: %')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKS_n_5sjKO1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images,labels = next(iter(test_loader)) \n",
    "images = images.view(-1, seq_dim, input_dim)[:20].to(device)\n",
    "labels = labels[:20].to(device)\n",
    "print(images.shape,labels.shape)\n",
    "a = model.forward(images,labels)\n",
    "print(a[0][0].cpu().numpy().shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYgvrAx93iHu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = a[0][0].cpu().numpy()\n",
    "pred1 = pred[:,1,:]\n",
    "# plt.imshow(pred1)\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfQqOvhSt4XW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = np.array(a[0][1])\n",
    "pred1 = pred[:,1,:]\n",
    "# plt.imshow(pred1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(images.cpu().numpy()[1,:,:])\n",
    "plt.subplot(122)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kjg2jRvu11M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images,labels = next(iter(test_loader)) \n",
    "images = images.view(-1, seq_dim, input_dim)[:20].to(device)\n",
    "images = images*torch.randn_like(images).gt(0.2)\n",
    "labels = labels[:20].to(device)\n",
    "print(images.shape,labels.shape)\n",
    "a = model.forward(images,labels)\n",
    "print(a[0][0].cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rClxoyS_3U7m",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = np.array(a[0][1])\n",
    "pred1 = pred[:,1,:]\n",
    "# plt.imshow(pred1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(images.cpu().numpy()[1,:,:])\n",
    "plt.subplot(122)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoGp6gRwC_7S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images,labels = next(iter(test_loader)) \n",
    "images = images.view(-1, seq_dim, input_dim)[:20].to(device)\n",
    "images = images*torch.randn_like(images).gt(0.1)\n",
    "labels = labels[:20].to(device)\n",
    "print(images.shape,labels.shape)\n",
    "a = model.forward(images,labels)\n",
    "print(a[0][0].cpu().numpy().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuhZ1QYNDETG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = np.array(a[0][1])\n",
    "pred1 = pred[:,1,:]\n",
    "# plt.imshow(pred1)\n",
    "plt.figure(figsize=(40,5))\n",
    "\n",
    "pred1 = pred[:,1,:]\n",
    "plt.subplot(251)\n",
    "plt.imshow(images.cpu().numpy()[1,:,:])\n",
    "plt.subplot(256)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "# plt.legend()\n",
    "\n",
    "pred1 = pred[:,3,:]\n",
    "plt.subplot(252)\n",
    "plt.imshow(images.cpu().numpy()[3,:,:])\n",
    "plt.subplot(257)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "# plt.legend()\n",
    "\n",
    "pred1 = pred[:,5,:]\n",
    "plt.subplot(253)\n",
    "plt.imshow(images.cpu().numpy()[5,:,:])\n",
    "plt.subplot(258)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "# plt.legend()\n",
    "\n",
    "pred1 = pred[:,7,:]\n",
    "plt.subplot(254)\n",
    "plt.imshow(images.cpu().numpy()[7,:,:])\n",
    "plt.subplot(259)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "# plt.legend()\n",
    "\n",
    "pred1 = pred[:,12,:]\n",
    "plt.subplot(255)\n",
    "plt.imshow(images.cpu().numpy()[12,:,:])\n",
    "plt.subplot(2,5,10)\n",
    "for i in range(10):\n",
    "    plt.plot(pred1[:,i],label=str(i))\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8PgiC6h2o1s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_cross_entropy(x, y):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    loss = log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 5\n",
    "nb_classes = 10\n",
    "x = torch.randn(batch_size, nb_classes, requires_grad=True)\n",
    "y = torch.randint(0, nb_classes, (batch_size,))\n",
    "\n",
    "loss_reference = criterion(x, y)\n",
    "loss = my_cross_entropy(x, y)\n",
    "\n",
    "print(loss_reference - loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFqsbSjF5pAI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def std_cross_entropy(x, y,std_mat):\n",
    "    log_prob = -1.0 * F.log_softmax(x, 1)\n",
    "    std_log_prob = std_mat*log_prob\n",
    "    print(std_log_prob.shape)\n",
    "    loss = std_log_prob.gather(1, y.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "batch_size = 5\n",
    "nb_classes = 10\n",
    "x = torch.randn(batch_size, nb_classes, requires_grad=True)\n",
    "y = torch.randint(0, nb_classes, (batch_size,))\n",
    "std_mat = torch.randn(batch_size, nb_classes)\n",
    "\n",
    "loss_reference = criterion(x, y)\n",
    "loss = std_cross_entropy(x, y,std_mat)\n",
    "\n",
    "print(loss_reference - loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NrEOrEi5uK-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}